{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class ConditionalDiffusionModel(nn.Module):\n",
    "    def __init__(self, input_size, weight_template_size):\n",
    "        super(ConditionalDiffusionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size + weight_template_size, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, input_size)\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        # Concatenate the image and weight template (condition)\n",
    "        x = torch.cat((x, condition), dim=-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.1601\n",
      "Epoch [2/50], Loss: 0.1000\n",
      "Epoch [3/50], Loss: 0.0619\n",
      "Epoch [4/50], Loss: 0.0592\n",
      "Epoch [5/50], Loss: 0.0422\n",
      "Epoch [6/50], Loss: 0.0371\n",
      "Epoch [7/50], Loss: 0.0374\n",
      "Epoch [8/50], Loss: 0.0350\n",
      "Epoch [9/50], Loss: 0.0422\n",
      "Epoch [10/50], Loss: 0.0283\n",
      "Epoch [11/50], Loss: 0.0286\n",
      "Epoch [12/50], Loss: 0.0312\n",
      "Epoch [13/50], Loss: 0.0284\n",
      "Epoch [14/50], Loss: 0.0210\n",
      "Epoch [15/50], Loss: 0.0255\n",
      "Epoch [16/50], Loss: 0.0311\n",
      "Epoch [17/50], Loss: 0.0326\n",
      "Epoch [18/50], Loss: 0.0221\n",
      "Epoch [19/50], Loss: 0.0299\n",
      "Epoch [20/50], Loss: 0.0303\n",
      "Epoch [21/50], Loss: 0.0210\n",
      "Epoch [22/50], Loss: 0.0305\n",
      "Epoch [23/50], Loss: 0.0432\n",
      "Epoch [24/50], Loss: 0.0302\n",
      "Epoch [25/50], Loss: 0.0235\n",
      "Epoch [26/50], Loss: 0.0204\n",
      "Epoch [27/50], Loss: 0.0263\n",
      "Epoch [28/50], Loss: 0.0245\n",
      "Epoch [29/50], Loss: 0.0235\n",
      "Epoch [30/50], Loss: 0.0272\n",
      "Epoch [31/50], Loss: 0.0240\n",
      "Epoch [32/50], Loss: 0.0274\n",
      "Epoch [33/50], Loss: 0.0285\n",
      "Epoch [34/50], Loss: 0.0200\n",
      "Epoch [35/50], Loss: 0.0196\n",
      "Epoch [36/50], Loss: 0.0244\n",
      "Epoch [37/50], Loss: 0.0245\n",
      "Epoch [38/50], Loss: 0.0208\n",
      "Epoch [39/50], Loss: 0.0238\n",
      "Epoch [40/50], Loss: 0.0256\n",
      "Epoch [41/50], Loss: 0.0231\n",
      "Epoch [42/50], Loss: 0.0265\n",
      "Epoch [43/50], Loss: 0.0190\n",
      "Epoch [44/50], Loss: 0.0208\n",
      "Epoch [45/50], Loss: 0.0240\n",
      "Epoch [46/50], Loss: 0.0218\n",
      "Epoch [47/50], Loss: 0.0217\n",
      "Epoch [48/50], Loss: 0.0187\n",
      "Epoch [49/50], Loss: 0.0218\n",
      "Epoch [50/50], Loss: 0.0227\n"
     ]
    }
   ],
   "source": [
    "class DiffusionTrainer:\n",
    "    def __init__(self, model, timesteps=1000):\n",
    "        self.model = model\n",
    "        self.timesteps = timesteps\n",
    "        self.beta = np.linspace(1e-4, 0.02, timesteps)  # Linear schedule\n",
    "        self.alpha = 1 - self.beta\n",
    "        self.alpha_bar = np.cumprod(self.alpha)\n",
    "\n",
    "    def sample_noise(self, shape):\n",
    "        return torch.randn(shape)\n",
    "\n",
    "    def q_sample(self, x_start, t):\n",
    "        noise = self.sample_noise(x_start.shape).to(x_start.device)\n",
    "        \n",
    "        # Correct broadcasting for alpha_bar_t\n",
    "        alpha_bar_t = torch.tensor(self.alpha_bar[t], dtype=torch.float32, device=x_start.device).view(-1, 1)\n",
    "        \n",
    "        return torch.sqrt(alpha_bar_t) * x_start + torch.sqrt(1 - alpha_bar_t) * noise\n",
    "\n",
    "    def loss_fn(self, x_noisy, t, x_start, condition):\n",
    "        predicted_x_start = self.model(x_noisy, condition)\n",
    "        return F.mse_loss(predicted_x_start, x_start)\n",
    "\n",
    "    def train(self, data_loader, optimizer, weight_templates, num_epochs=100):\n",
    "        for epoch in range(num_epochs):\n",
    "            for i, (x_start, idx) in enumerate(data_loader):\n",
    "                t = torch.randint(0, self.timesteps, (x_start.size(0),)).to(x_start.device)\n",
    "                x_noisy = self.q_sample(x_start, t)\n",
    "                condition = weight_templates[idx].to(x_start.device)\n",
    "\n",
    "                loss = self.loss_fn(x_noisy, t, x_start, condition)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Input image size and weight template size\n",
    "input_size = 150 * 150 * 1  # Example for 64x64 RGB images\n",
    "weight_template_size = 150  # Assuming your weight template is 256-dimensional\n",
    "\n",
    "# Initialize model\n",
    "model = ConditionalDiffusionModel(input_size, weight_template_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "trainer = DiffusionTrainer(model)\n",
    "\n",
    "# Example: Data loading (assume images and weight templates are in NumPy format)\n",
    "# Here `images.npy` contains the training images and `weight_templates.npy` contains the corresponding weight templates.\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load images and weight templates\n",
    "images = np.load(\"/Users/gvssriram/Desktop/projects-internship/PalmGenAI/Datasets/IITD Palmprint V1/Preprocessed/Left/X_train.npy\")  # Shape: (N, 64, 64, 3)\n",
    "weight_templates = np.load(\"/Users/gvssriram/Desktop/projects-internship/PalmGenAI/Datasets/IITD Palmprint V1/Preprocessed/Left/X_train_pca.npy\")  # Shape: (N, 256)\n",
    "\n",
    "# Preprocessing\n",
    "images = torch.tensor(images).float().view(-1, input_size)  # Flatten images\n",
    "weight_templates = torch.tensor(weight_templates).float()\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TensorDataset(images, torch.arange(images.shape[0]))  # Pass indices for weight templates\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Train the model\n",
    "trainer.train(data_loader, optimizer, weight_templates, num_epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9q/_q4q_3y536j3lx6cc49vl0840000gn/T/ipykernel_65800/82786077.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  user_weight_template = torch.tensor(user_weight_template).float().unsqueeze(0)\n"
     ]
    }
   ],
   "source": [
    "class DiffusionSampler:\n",
    "    def __init__(self, model, timesteps=1000):\n",
    "        self.model = model\n",
    "        self.timesteps = timesteps\n",
    "        self.beta = np.linspace(1e-4, 0.02, timesteps)  # Linear schedule\n",
    "        self.alpha = 1 - self.beta\n",
    "        self.alpha_bar = np.cumprod(self.alpha)\n",
    "\n",
    "    def p_sample(self, x, t, condition):\n",
    "        predicted_x_start = self.model(x, condition)\n",
    "        alpha_bar_t = torch.tensor(self.alpha_bar[t], dtype=torch.float32, device=x.device)\n",
    "        alpha_bar_t_prev = torch.tensor(self.alpha_bar[t-1], dtype=torch.float32, device=x.device) if t > 0 else torch.tensor(1.0, dtype=torch.float32, device=x.device)\n",
    "\n",
    "        noise = torch.randn_like(x)\n",
    "        return predicted_x_start * torch.sqrt(alpha_bar_t_prev) + noise * torch.sqrt(1 - alpha_bar_t_prev)\n",
    "\n",
    "    def sample(self, condition, img_shape=(150, 150, 1)):\n",
    "        x = torch.randn((1, np.prod(img_shape))).to(condition.device)\n",
    "        for t in reversed(range(self.timesteps)):\n",
    "            x = self.p_sample(x, t, condition)\n",
    "        return x.view(img_shape)\n",
    "\n",
    "# Initialize sampler\n",
    "sampler = DiffusionSampler(model)\n",
    "\n",
    "# Load weight templates for the user\n",
    "user_weight_template = weight_templates[0]  # Shape: (256,)\n",
    "user_weight_template = torch.tensor(user_weight_template).float().unsqueeze(0)\n",
    "\n",
    "# Generate deepfake\n",
    "generated_image = sampler.sample(user_weight_template)\n",
    "generated_image = generated_image.detach().cpu().numpy()\n",
    "\n",
    "# Reshape to original image dimensions and save or display\n",
    "generated_image = generated_image.reshape(150, 150, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Displaying the original and generated images side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Original image\n",
    "axes[0].imshow(images[0].reshape(150, 150), cmap='gray')\n",
    "axes[0].set_title(\"Original Image\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Generated image\n",
    "axes[1].imshow(generated_image.reshape(150, 150), cmap='gray')\n",
    "axes[1].set_title(\"Generated Image\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Show the plot\n",
    "plt.savefig('diffusion.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio_project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
